{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef9230c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from decimal import Decimal\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "############\n",
    "# COMPONENTS\n",
    "############\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=self.hidden_dim, \n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=embedding_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        #print(f'ENCODER input dim: {x.shape}')\n",
    "        x = x.reshape((batch_size, self.seq_len, self.n_features))\n",
    "        #print(f'ENCODER reshaped dim: {x.shape}')\n",
    "        x, (h_n, c_n) = self.rnn1(x)\n",
    "        #print('h_n = ',h_n.shape)\n",
    "        #print('c_n = ',c_n.shape)\n",
    "        #print(f'ENCODER output rnn1 dim: {x.shape}')\n",
    "        x, (hidden_n, _) = self.rnn2(x)\n",
    "        #print(f'ENCODER output rnn2 dim: {x.shape}')\n",
    "        #print(f'ENCODER hidden_n rnn2 dim: {hidden_n.shape}')\n",
    "        #print(f'ENCODER hidden_n wants to be reshaped to : {(batch_size, self.embedding_dim)}')\n",
    "        return hidden_n.reshape((batch_size, self.embedding_dim))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim=64, n_features=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=input_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        #print(f'DECODER input dim: {x.shape}')\n",
    "        x = x.repeat(self.seq_len, self.n_features) # todo testare se funziona con più feature\n",
    "        #print(f'DECODER repeat dim: {x.shape}')\n",
    "        x = x.reshape((batch_size, self.seq_len, self.input_dim))\n",
    "        #print(f'DECODER reshaped dim: {x.shape}')\n",
    "        x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "        #print(f'DECODER output rnn1 dim:/ {x.shape}')\n",
    "        x, (hidden_n, cell_n) = self.rnn2(x)\n",
    "        #print(f'DECODER output rnn2 dim:/ {x.shape}')\n",
    "        x = x.reshape((batch_size, self.seq_len, self.hidden_dim))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "######\n",
    "# MAIN\n",
    "######\n",
    "\n",
    "\n",
    "class RecurrentAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64, device='cuda', batch_size=10):\n",
    "        super(RecurrentAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "        self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3bcc4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 140)\n",
      "(40, 140)\n",
      "(1400, 140, 1)\n",
      "(40, 140, 1)\n",
      "1190 210 210\n",
      "train_indices:  980\n",
      "val_indices:  210\n",
      "test_indices:  210\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "dataset = np.load('../data/data_process.npz') \n",
    "dataset_normal = dataset['normal'][:,0:140]\n",
    "dataset_anomaly = dataset['anomaly'][:,0:140]\n",
    "print(dataset_normal.shape)\n",
    "print(dataset_anomaly.shape)\n",
    "dataset_normal = np.expand_dims(dataset_normal,axis=2) #升維\n",
    "dataset_anomaly = np.expand_dims(dataset_anomaly,axis=2) #升維\n",
    "print(dataset_normal.shape)\n",
    "print(dataset_anomaly.shape)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seq_len, n_features = 140, 1\n",
    "batch_size = 10\n",
    "\n",
    "################################\n",
    "validation_split = test_split = 0.15\n",
    "train_split = 0.85\n",
    "random_seed = 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset_normal)\n",
    "indices = list(range(dataset_size)) \n",
    "dataset_anomaly_size = len(dataset_anomaly)\n",
    "anomaly_indices = list(range(dataset_anomaly_size))\n",
    "\n",
    "\n",
    "train_split = int(train_split * dataset_size)\n",
    "test_split = int(np.floor(test_split * dataset_size))\n",
    "validation_split = int(np.floor(validation_split * dataset_size))\n",
    "print(train_split,test_split,validation_split)\n",
    "\n",
    "# suffling\n",
    "#np.random.seed(random_seed)\n",
    "#np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[0:train_split], indices[train_split:]\n",
    "train_indices, test_indices = train_indices[:train_split-test_split] ,train_indices[train_split-test_split:]\n",
    "\n",
    "\n",
    "print('train_indices: ', len(train_indices))\n",
    "print('val_indices: ', len(val_indices))\n",
    "print('test_indices: ', len(test_indices))\n",
    "\n",
    "\n",
    "# check all splits have no intersections\n",
    "assert not [value for value in train_indices if value in test_indices]\n",
    "assert not [value for value in train_indices if value in val_indices]\n",
    "assert not [value for value in val_indices if value in test_indices]\n",
    "##############################\n",
    "\n",
    "\n",
    "model = RecurrentAutoencoder(seq_len, n_features=n_features, embedding_dim=64, device=device, batch_size=batch_size) \n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SequentialSampler(train_indices) \n",
    "\n",
    "valid_sampler = SubsetRandomSampler(val_indices) \n",
    "test_sampler = SubsetRandomSampler(test_indices) \n",
    "anomaly_sampler = SequentialSampler(anomaly_indices)  \n",
    "\n",
    "train_loader =DataLoader(dataset_normal,sampler=train_sampler, batch_size=batch_size)\n",
    "validation_loader = DataLoader(dataset_normal, batch_size=batch_size, sampler=valid_sampler)  \n",
    "test_loader = DataLoader(dataset_normal, sampler=test_sampler, batch_size=10)  \n",
    "anomaly_loader = DataLoader(dataset_anomaly,sampler=anomaly_sampler ,batch_size=1)\n",
    "\n",
    "# start training\n",
    "n_epochs = 100  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss(reduction='mean').to(device) \n",
    "history = dict(train=[], val=[])\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = 10000.0\n",
    "\n",
    "for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "    model = model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "    anomaly_losses = []\n",
    "    \n",
    "    for i, seq_true in enumerate(train_loader):  \n",
    "        optimizer.zero_grad()\n",
    "        seq_true = seq_true.float().cuda()\n",
    "        seq_pred = model(seq_true)\n",
    "        loss = criterion(seq_pred, seq_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "   \n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # validation steps\n",
    "        for i, seq_true in enumerate(validation_loader):\n",
    "            seq_true = seq_true.to(device)\n",
    "            seq_pred = model(seq_true)\n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "        # normal_test steps\n",
    "        for i, seq_true in enumerate(test_loader):\n",
    "            #print(i)\n",
    "            #print(seq_true[0])\n",
    "            seq_true = seq_true.to(device)\n",
    "            seq_pred = model(seq_true)\n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        # anomaly_test steps\n",
    "        for i, seq_true in enumerate(anomaly_loader):\n",
    "            seq_true = seq_true.to(device)\n",
    "            seq_pred = model(seq_true)\n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "            anomaly_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    test_loss = np.mean(test_losses)\n",
    "    #\n",
    "    anomaly_loss = np.mean(anomaly_losses)\n",
    "    #自己存的\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        torch.save(model, '../data/best_model_{}.pt')\n",
    "    torch.save(model, '../data/last_model_{}.pt')\n",
    "    \n",
    "    history['train'].append(train_loss)\n",
    "    print(f'Epoch {epoch}: train loss {train_loss} {\" \"*6} val loss {val_loss} {\" \"*6} test loss {test_loss} {\" \"*6} anomaly loss {anomaly_loss}')\n",
    "\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea39be48",
   "metadata": {},
   "source": [
    "預測資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abbd4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_batch_size = 10 \n",
    "eval_loss = nn.MSELoss(reduction='none')  \n",
    "\n",
    "# load trained model\n",
    "checkpoint_path = '../data/last_model_{}.pt'\n",
    "model = torch.load(checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "output_data = list()\n",
    "anomality = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae13aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):  \n",
    "        #print(i)\n",
    "        #print(data[0])\n",
    "        img = data.float().cuda()\n",
    "        output = model(img)\n",
    "        sum_loss = eval_loss(output, img).sum([1])\n",
    "        anomality.append(sum_loss)\n",
    "        output_data.append(output)\n",
    "        \n",
    "    for i, data in enumerate(anomaly_loader):  \n",
    "        img = data.float().cuda()\n",
    "        output = model(img)\n",
    "        sum_loss = eval_loss(output, img).sum([1])\n",
    "        anomality.append(sum_loss)\n",
    "        output_data.append(output) \n",
    "\n",
    "    \n",
    "data_pre = torch.cat(anomality, axis=0)\n",
    "data_pre = data_pre.cpu().detach().numpy()\n",
    "\n",
    "np.savez('../data/data_pre.npz',normal=data_pre[:210], anomaly=data_pre[210:])    \n",
    "print(data_pre[:210].shape)\n",
    "print(data_pre[210:].shape)         \n",
    "       \n",
    "output_data = torch.cat(output_data, axis=0)\n",
    "output_data = output_data.cpu().detach().numpy()\n",
    "np.savez('../data/output_data.npz',normal=output_data[:210],anomaly=output_data[210:])\n",
    "print(output_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
